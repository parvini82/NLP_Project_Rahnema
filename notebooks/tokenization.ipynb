{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67eddad764906f5",
   "metadata": {},
   "source": [
    "<div dir='rtl' style=\"font-family: Vazirmatn, IRANSans, 'Segoe UI', Tahoma, sans-serif; line-height: 1.9; font-size: 16px;\">\n",
    "\n",
    "## Byte-Pair Encoding (BPE)\n",
    "\n",
    "BPE یا Byte-Pair Encoding ابتدا به‌عنوان روش فشرده‌سازی معرفی شد و بعدها در پردازش زبان طبیعی برای شکستن کلمات به زیرواحدها (subword) به‌کار رفت. ایده‌ی اصلی: به‌جای نگهداری هر کلمه به‌صورت کامل، واژگان را از قطعات پرتکرار می‌سازیم تا اندازه‌ی واژگان محدود و مسئله‌ی نشانه‌ی ناشناخته (UNK) کاهش یابد. این رویکرد برای زبان‌های با واژه‌های طولانی یا ترکیبی سودمند است.\n",
    "\n",
    "### الگوریتم BPE (خلاصه‌ی گام‌به‌گام)\n",
    "- **گام ۱: آماده‌سازی**: شکستن متن آموزشی به کلمات و سپس کاراکترها.\n",
    "  - مثال: `low → l o w`، `lower → l o w e r`\n",
    "- **گام ۲: شمارش جفت‌ها**: شمارش فراوانی تمام جفت‌های متوالی کاراکترها در کل پیکره.\n",
    "- **گام ۳: ادغام پرتکرارترین جفت**: پرتکرارترین جفت را با یک توکن جدید جایگزین می‌کنیم.\n",
    "  - مثال: اگر `l o` پرتکرارترین باشد: `l o w → lo w`، `l o w e r → lo w e r`\n",
    "- **گام ۴: تکرار**: گام‌های ۲ و ۳ را تا رسیدن به اندازه‌ی واژگان هدف یا نبود جفت پرتکرار ادامه می‌دهیم.\n",
    "- **گام ۵: ثبت قوانین ادغام**: تمام ادغام‌ها را به‌صورت «merge rules» ذخیره می‌کنیم.\n",
    "- **گام ۶: توکنیزه‌سازی متن جدید**: اعمال ترتیبی قوانین ادغام برای شکستن واژه‌های جدید به subwordهای رایج.\n",
    "  - نمونه: `text: lowest`، `merge rules: lo, we, st`، نتیجه: `lo w e st`\n",
    "\n",
    "### مزایا\n",
    "- **سادگی و سرعت آموزش**\n",
    "- **کاهش UNK** (به‌ویژه با حالت مبتنی بر بایت)\n",
    "- **مناسب برای زبان‌های با واژه‌های طولانی/ترکیبی**\n",
    "- **واژگان کوچک‌تر نسبت به رویکرد واژه‌محور کامل**\n",
    "\n",
    "### معایب\n",
    "- **صرفاً مبتنی بر فراوانی**؛ معنای زبانی را لحاظ نمی‌کند.\n",
    "- **احتمال تولید قطعات غیرمعنادار** برای واژه‌های نادر.\n",
    "- **حساسیت به نویسه/فاصله**، برای زبان‌های با صرف پیچیده همیشه بهینه نیست.\n",
    "\n",
    "### ابزارها و مدل‌های مرتبط\n",
    "- **کتابخانه‌ها**: `subword-nmt`، Hugging Face `tokenizers`، `sentencepiece`\n",
    "- **مدل‌ها**: GPT-2 و خانواده‌ی GPT، RoBERTa، BART، برخی سیستم‌های ترجمه مانند OpenNMT\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11677dabf56e3d32",
   "metadata": {},
   "source": [
    "<div dir='rtl' style=\"font-family: Vazirmatn, IRANSans, 'Segoe UI', Tahoma, sans-serif; line-height: 1.9; font-size: 16px;\">\n",
    "\n",
    "## WordPiece\n",
    "\n",
    "WordPiece توسط گوگل معرفی شد؛ ابتدا در ترجمه‌ی ماشینی و سپس در BERT فراگیر شد. تفاوت کلیدی با BPE این است که افزودن زیرواحد جدید براساس بهبود «احتمال مدل زبانی» انجام می‌شود، نه صرفاً فراوانی وقوع.\n",
    "\n",
    "### الگوریتم (خلاصه)\n",
    "1. شروع از واژگان مبتنی بر کاراکتر.\n",
    "2. ساخت کاندید برای افزودن به واژگان.\n",
    "3. ارزیابی هر کاندید براساس بهبود احتمال متن تحت مدل زبانی.\n",
    "4. افزودن بهترین کاندید به واژگان.\n",
    "5. تکرار تا رسیدن به اندازه‌ی هدف واژگان.\n",
    "\n",
    "### توکنیزه‌سازی متن جدید\n",
    "- استفاده از راهبرد greedy longest-match.\n",
    "- در BERT، زیرواحدهای درون‌کلمه‌ای با پیشوند `##` مشخص می‌شوند.\n",
    "  - نمونه: `playing → play + ##ing`\n",
    "\n",
    "### مزایا\n",
    "- **انسجام معنایی بهتر** نسبت به BPE در سطح زیرواحد.\n",
    "- **مناسب برای زبان‌های با فاصله‌گذاری مشخص** بین کلمات.\n",
    "\n",
    "### معایب\n",
    "- **پیچیدگی آموزشی بیشتر** از BPE.\n",
    "- در نبود fallback ممکن است با واژه‌های جدید مشکل رخ دهد.\n",
    "\n",
    "### ابزارها و مدل‌های مرتبط\n",
    "- **کتابخانه‌ها**: Hugging Face `transformers`، Hugging Face `tokenizers`، TensorFlow Text\n",
    "- **مدل‌ها**: BERT، mBERT، DistilBERT و دیگر مدل‌های گوگل\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c410c6e69f3f8da",
   "metadata": {},
   "source": [
    "<div dir='rtl' style=\"font-family: Vazirmatn, IRANSans, 'Segoe UI', Tahoma, sans-serif; line-height: 1.9; font-size: 16px;\">\n",
    "\n",
    "## SentencePiece\n",
    "\n",
    "SentencePiece توسط گوگل توسعه داده شد و بر خلاف بسیاری از روش‌ها، به جداسازی اولیه‌ی متن بر اساس فاصله نیاز ندارد؛ بنابراین می‌توان آن را مستقیماً روی متن خام به‌کار برد. این ویژگی برای زبان‌های بدون فاصله بین کلمات (مانند چینی و ژاپنی) و نیز مدل‌های چندزبانه بسیار مفید است.\n",
    "\n",
    "SentencePiece دو حالت اصلی دارد:\n",
    "1. **BPE mode**: مشابه الگوریتم BPE عمل می‌کند.\n",
    "2. **Unigram mode**: مبتنی بر یک مدل احتمالاتی است.\n",
    "\n",
    "### الگوریتم Unigram (خلاصه)\n",
    "1. ساخت مجموعه‌ی بزرگی از زیرواحدهای کاندید.\n",
    "2. تخصیص احتمال اولیه به هر زیرواحد.\n",
    "3. به‌روزرسانی احتمال‌ها با EM برای بهینه‌سازی تقسیم‌بندی متن.\n",
    "4. حذف واحدهای کم‌استفاده برای کوچک‌سازی واژگان.\n",
    "5. انتخاب بهترین تقسیم‌بندی نهایی با الگوریتم Viterbi.\n",
    "\n",
    "### ویژگی‌ها\n",
    "- **بی‌نیاز از پیش‌جداسازی مبتنی بر فاصله**\n",
    "- **کارآمد برای زبان‌های بدون فاصله**\n",
    "- **امکان تولید تقسیم‌بندی‌های احتمالاتی متنوع**\n",
    "\n",
    "### معایب\n",
    "- **آموزش کندتر از BPE** در حالت Unigram.\n",
    "- **نیاز به تنظیمات دقیق‌تر** در طول آموزش.\n",
    "\n",
    "### نکته\n",
    "- نشانه‌ی ویژه‌ی `▁` برای نمایش فاصله‌ی میان کلمات به‌کار می‌رود.\n",
    "\n",
    "### ابزارها و مدل‌های مرتبط\n",
    "- **کتابخانه‌ها**: `sentencepiece`، Hugging Face Transformers و Tokenizers\n",
    "- **مدل‌ها**: T5، mT5، XLM-R، مدل‌های چندزبانه‌ی گوگل\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "822fad1c6f622a8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T06:20:37.982039Z",
     "start_time": "2025-09-24T06:20:37.979869Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer,WordPieceTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97531124dfd800b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T06:19:05.610743Z",
     "start_time": "2025-09-24T06:19:05.605534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokens: ['This', 'is', 'a', 'simple', 'example', 'to', 'demon', 'st', 'rat', 'e', 'BPE', 'token', 'ization', '.']\n",
      "Token IDs: [46, 23, 6, 42, 48, 27, 47, 41, 40, 8, 45, 44, 49, 1]\n",
      "Number of tokens: 14\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"This is a simple example to demonstrate BPE tokenization.\"\n",
    "\n",
    "# Create a BPE tokenizer\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Pre-tokenizer splits text into words with space\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Trainer defines how BPE merges subwords to build the vocabulary\n",
    "trainer = BpeTrainer(vocab_size=50, min_frequency=1, special_tokens=[\"[UNK]\"])\n",
    "\n",
    "# Train the tokenizer on the sample text\n",
    "tokenizer.train_from_iterator([text], trainer=trainer)\n",
    "\n",
    "# Encode the text using the trained tokenizer\n",
    "output = tokenizer.encode(text)\n",
    "\n",
    "# Print the tokens, token IDs, and number of tokens\n",
    "print(\"Tokens:\", output.tokens)\n",
    "print(\"Token IDs:\", output.ids)\n",
    "print(\"Number of tokens:\", len(output.tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9367f66118916bc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T06:20:39.988356Z",
     "start_time": "2025-09-24T06:20:39.983846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokens: ['Th', '##i', '##s', 'i', '##s', 'a', 's', '##i', '##mple', 'ex', '##a', '##mple', 'to', 'de', '##m', '##on', '##s', '##t', '##r', '##at', '##e', 'Wo', '##r', '##d', '##P', '##i', '##e', '##c', '##e', 'to', '##k', '##e', '##n', '##i', '##z', '##at', '##i', '##on', '.']\n",
      "Token IDs: [46, 27, 22, 10, 22, 5, 18, 27, 45, 49, 29, 45, 40, 48, 32, 41, 22, 30, 35, 42, 25, 47, 35, 36, 37, 27, 25, 38, 25, 40, 24, 25, 26, 27, 28, 42, 27, 41, 1]\n",
      "Number of tokens: 39\n"
     ]
    }
   ],
   "source": [
    "#sample text\n",
    "text = \"This is a simple example to demonstrate WordPiece tokenization.\"\n",
    "\n",
    "# Create a WordPiece tokenizer\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Pre-tokenizer splits text into words based on whitespace before applying WordPiece\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Trainer defines how WordPiece builds subwords vocabulary\n",
    "trainer = WordPieceTrainer(vocab_size=50, min_frequency=1, special_tokens=[\"[UNK]\"])\n",
    "\n",
    "# Train the tokenizer on the sample text\n",
    "tokenizer.train_from_iterator([text], trainer=trainer)\n",
    "\n",
    "# Encode the text using the trained tokenizer\n",
    "output = tokenizer.encode(text)\n",
    "\n",
    "# Print the tokens, token IDs, and number of tokens\n",
    "print(\"Tokens:\", output.tokens)\n",
    "print(\"Token IDs:\", output.ids)\n",
    "print(\"Number of tokens:\", len(output.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71820a1819429fe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-24T06:22:57.549318Z",
     "start_time": "2025-09-24T06:22:57.530890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁', 'Th', 'is', '▁', 'is', '▁a', '▁', 'si', 'mple', '▁', 'exa', 'mple', '▁to', '▁', 'dem', 'on', 'st', 'r', 'ate', '▁S', 'ent', 'en', 'ce', 'Pie', 'ce', '▁to', 'k', 'en', 'iz', 'ati', 'on', '.']\n",
      "Token IDs: [29, 14, 6, 29, 6, 21, 29, 18, 12, 29, 27, 12, 11, 29, 25, 9, 19, 47, 23, 20, 26, 3, 5, 22, 5, 11, 46, 3, 17, 24, 9, 40]\n",
      "Number of tokens: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: sample.txt\n",
      "  input_format: \n",
      "  model_prefix: spm_model\n",
      "  model_type: BPE\n",
      "  vocab_size: 50\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: sample.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 1 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=68\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=22\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 1 sentences.\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 1\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 9\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=20 all=91 active=69 piece=Pie\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: spm_model.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: spm_model.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Sample text for tokenization\n",
    "text = \"This is a simple example to demonstrate SentencePiece tokenization.\"\n",
    "\n",
    "# First, we need to save the text to a temporary file for training\n",
    "with open(\"sample.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "# Train a SentencePiece model with a small vocabulary size for demonstration\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"sample.txt\", \n",
    "    model_prefix=\"spm_model\", \n",
    "    vocab_size=50, \n",
    "    model_type=\"bpe\"  # can be 'bpe', 'unigram', 'char', or 'word'\n",
    ")\n",
    "\n",
    "# Load the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"spm_model.model\")\n",
    "\n",
    "# Encode the text\n",
    "tokens = sp.encode(text, out_type=str)\n",
    "token_ids = sp.encode(text, out_type=int)\n",
    "\n",
    "# Print tokens and IDs\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Number of tokens:\", len(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
